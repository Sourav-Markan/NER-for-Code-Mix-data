{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Named Entity Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook describes named entity recognition for code mix data experimenting with different machine learning classification algorithms with word, character and lexical features. The algorithms used for NER in this notebook are Decision tree, Long Short-Term Memory (LSTM), and Conditional Random Field (CRF). I have implemented the model using scikit-learn and keras library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA-SET "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we are training our NER model for code mix data mainly Hindi-English. We have taken Hindi-English code-mixed tweets containing tweets from last 8 years (on themes like legislative issues, sports, etc from the Indian subcontinent point of view). For more information of Dataset refer to http://aclweb.org/anthology/W18-2405"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv,sys,re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the NER dataset into a Pandas DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, the data is loaded into a Pandas DataFrame. This can be done easily using the read_csv function, specifying that the separator is a comma. It's also useful to keep the blank lines, which are helpful later for determining the sentence breaks.\n",
    "\n",
    "Once the data is loaded into a DataFrame, Now we have easy access to columns allows a couple of useful things to be done - group the data by the \"ne\" column to see the distributions of each tag, and extract the classes (disregarding 'O' and blank lines with NaN values) as a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     tag  counts\n",
      "0  B-Loc     762\n",
      "1  B-Org    1432\n",
      "2  B-Per    2138\n",
      "3  I-Loc      31\n",
      "4  I-Org      90\n",
      "5  I-Per     554\n",
      "6  Other   63499\n"
     ]
    }
   ],
   "source": [
    "\n",
    "ner_data = pd.read_csv(\"annotatedData.csv\", sep=\",\", header=None, skip_blank_lines=False, encoding=\"utf-8\")\n",
    "ner_data = ner_data[1:]\n",
    "ner_data.columns = [\"sen_num\", \"word\", \"tag\"]\n",
    "\n",
    "# Explore thbe distribution of NE tags in the dataset\n",
    "tag_distribution = ner_data.groupby(\"tag\").size().reset_index(name='counts')\n",
    "print(tag_distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['B-Per', 'Other', 'B-Org', 'I-Org', 'B-Loc', 'I-Per', 'I-Loc']\n"
     ]
    }
   ],
   "source": [
    "# Extract the useful classes (not 'O' or NaN values) as a list\n",
    "classes = list(filter(lambda x: x not in [\"O\", np.nan], list(ner_data[\"tag\"].unique())))\n",
    "\n",
    "print(classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features corresponding to every word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The feature set consists of word, character and lexical level information like char N-Grams of Gram size 2 and 3 for suffixes, patterns for punctuation, emoticons, numbers, numbers inside strings, social media specific characters like ‘#’, ‘@’ and\n",
    "also previous tag information, and the same all features of the previous and next tokens are used as context features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    " def word2features(sent, i,word2idx,tag2idx,word2Suff2idx,word3Suff2idx,wordLower2idx,binaryIdx):\n",
    "        word = sent[i][0]  \n",
    "        features = {\n",
    "            'bias': 1.0,\n",
    "            'word': word2idx[word],\n",
    "            'word.lower()': wordLower2idx[word.lower()],\n",
    "            'word[-3:]': word3Suff2idx[word[-3:]],\n",
    "            'word[-2:]': word2Suff2idx[word[-2:]],\n",
    "            'word.isupper()': binaryIdx[str(word.isupper())],\n",
    "            'word.istitle()': binaryIdx[str(word.istitle())],\n",
    "            'word.isdigit()': binaryIdx[str(word.isdigit())],\n",
    "            'word.startsWith#()': binaryIdx[str(word.startswith(\"#\"))],\n",
    "            'word.startsWith@()': binaryIdx[str(word.startswith(\"@\"))],\n",
    "            'word.1stUpper()': binaryIdx[str(word[0].isupper())],\n",
    "            'word.isAlpha()': binaryIdx[str(word.isalpha())],\n",
    "            'word.Tag': tag2idx[sent[i][1]],\n",
    "        }\n",
    "        if i > 0:\n",
    "            word1 = sent[i-1][0]\n",
    "            features.update({\n",
    "                '-1:word': word2idx[word1],\n",
    "                '-1:word.lower()': wordLower2idx[word1.lower()],\n",
    "                '-1:word.istitle()': binaryIdx[str(word1.istitle())],\n",
    "                '-1:word.isupper()': binaryIdx[str(word1.isupper())],\n",
    "                '-1:word.istitle()': binaryIdx[str(word1.istitle())],\n",
    "                '-1:word.isdigit()': binaryIdx[str(word1.isdigit())],\n",
    "                '-1:word.startsWith#()': binaryIdx[str(word1.startswith(\"#\"))],\n",
    "                '-1:word.startsWith@()': binaryIdx[str(word1.startswith(\"@\"))],\n",
    "                '-1:word.1stUpper()': binaryIdx[str(word1[0].isupper())],\n",
    "                '-1:word.isAlpha()': binaryIdx[str(word1.isalpha())],\n",
    "            })\n",
    "        else:\n",
    "            features['BOS'] = binaryIdx[str(\"True\")]\n",
    "\n",
    "        if i < len(sent)-1:\n",
    "            word1 = sent[i+1][0]\n",
    "            features.update({\n",
    "                '+1:word': word2idx[word1],\n",
    "                '+1:word.lower()': wordLower2idx[word1.lower()],\n",
    "                '+1:word.istitle()': binaryIdx[str(word1.istitle())],\n",
    "                '+1:word.isupper()': binaryIdx[str(word1.isupper())],\n",
    "                '+1:word.istitle()': binaryIdx[str(word1.istitle())],\n",
    "                '+1:word.isdigit()': binaryIdx[str(word1.isdigit())],\n",
    "                '+1:word.startsWith#()': binaryIdx[str(word1.startswith(\"#\"))],\n",
    "                '+1:word.startsWith@()': binaryIdx[str(word1.startswith(\"@\"))],\n",
    "                '+1:word.1stUpper()': binaryIdx[str(word1[0].isupper())],\n",
    "                '+1:word.isAlpha()': binaryIdx[str(word1.isalpha())],\n",
    "            })\n",
    "        else:\n",
    "            features['EOS'] = binaryIdx[str(\"True\")]\n",
    "\n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SentenceGetter(object):\n",
    "            \n",
    "    def __init__(self, data):\n",
    "        self.n_sent = 1\n",
    "        self.data = data\n",
    "        self.empty = False\n",
    "        agg_func = lambda s: [(w, t) for w, t in zip(s[\"Word\"].values.tolist(),\n",
    "                                                           s[\"Tag\"].values.tolist())]\n",
    "        self.grouped = self.data.groupby(\"Sent\").apply(agg_func)\n",
    "        self.sentences = [s for s in self.grouped]\n",
    "    \n",
    "    def get_next(self):\n",
    "        try:\n",
    "            s = self.grouped[\"Sentence: {}\".format(self.n_sent)]\n",
    "            self.n_sent += 1\n",
    "            return s\n",
    "        except:\n",
    "            return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent2features(sent,word2idx,tag2idx,word2Suff2idx,word3Suff2idx,wordLower2idx,binaryIdx):\n",
    "#    print (sent)\n",
    "    return list(word2features(sent, i,word2idx,tag2idx,word2Suff2idx,word3Suff2idx,wordLower2idx,binaryIdx) for i in range(len(sent)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent2labels(sent):\n",
    "    return [label for token, label in sent]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Features vector corresponding to each sentence uses following features:\n",
    "- Character N-Grams\n",
    "- Word N-Gram\n",
    "- Capitalization\n",
    "- Mentions and Hashtags\n",
    "- Numbers in String\n",
    "- Previous Word Tag\n",
    "- Common Symbols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numericFeatures():\n",
    "    data = pd.read_csv(\"annotatedData.csv\", encoding=\"latin1\")\n",
    "    data = data.fillna(method=\"ffill\")\n",
    "\n",
    "    words = list(set(data[\"Word\"].values))\n",
    "    words.append(\"ENDPAD\")\n",
    "    tags = list(set(data[\"Tag\"].values))\n",
    "#     print (words)\n",
    "    print (tags)\n",
    "\n",
    "\n",
    "\n",
    "    max_len = 50\n",
    "    word2idx = {w: i for i, w in enumerate(words)}\n",
    "    tag2idx = {t: i for i, t in enumerate(tags)}\n",
    "    word2Suff2idx = {w[-2:]: i for i, w in enumerate(words)}\n",
    "    word3Suff2idx = {w[-3:]: i for i, w in enumerate(words)}\n",
    "    wordLower2idx = {w.lower(): i for i, w in enumerate(words)}\n",
    "    binaryIdx = {\"True\": 1, \"False\": 0}\n",
    "\n",
    "#     print (binaryIdx[str(\"False\")])\n",
    "\n",
    "    # X = [[binaryIdx[str(w[5]] for w in s] for s in features]\n",
    "\n",
    "\n",
    "    getter = SentenceGetter(data)\n",
    "    # sent = getter.get_next()\n",
    "    sentences = getter.sentences\n",
    "    #print (sentences)\n",
    "    \n",
    "    X = list(sent2features(s,word2idx,tag2idx,word2Suff2idx,word3Suff2idx,wordLower2idx,binaryIdx) for s in sentences)\n",
    "    #print (pd.DataFrame(X[0]))\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I-Per', 'B-Per', 'I-Org', 'B-Loc', 'Other', 'I-Loc', 'B-Org']\n",
      "33\n"
     ]
    }
   ],
   "source": [
    "featureVec = numericFeatures()\n",
    "csv_columns = ['+1:word', '+1:word.1stUpper()', '+1:word.isAlpha()', '+1:word.isdigit()', '+1:word.istitle()','+1:word.isupper()', '+1:word.lower()', '+1:word.startsWith#()', '+1:word.startsWith@()', 'BOS', '-1:word', '-1:word.1stUpper()', '-1:word.isAlpha()', '-1:word.isdigit()', '-1:word.istitle()', '-1:word.isupper()','-1:word.lower()', '-1:word.startsWith#()', '-1:word.startsWith@()', 'EOS', 'bias', 'word', 'word.1stUpper()', 'word.isAlpha()', 'word.isdigit()', 'word.istitle()','word.isupper()', 'word.lower()', 'word.startsWith#()', 'word.startsWith@()', 'word[-2:]', 'word[-3:]', 'word.Tag']\n",
    "print(len(csv_columns))\n",
    "\n",
    "with open('featureVector.csv', 'w') as ofile:\n",
    "    writer = csv.DictWriter(ofile, csv_columns)\n",
    "    writer.writeheader()\n",
    "    for sen in featureVec:\n",
    "        for word in sen:\n",
    "            # print d\n",
    "            writer.writerow(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language Model Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we have experimented different classifier for identifying language. Further we will determine the effect of each feature and\n",
    "parameter of different models by performing several experiments with some set of feature vectors at a time and all at a time simultaneously changing the values of the parameters of our language classifier models like maximum depth of the tree for Decision tree model, etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/akshat14714/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for Decision tree..\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       I-Loc       0.38      0.39      0.39       153\n",
      "       B-Org       0.67      0.65      0.66       645\n",
      "       I-Per       0.20      0.22      0.21        23\n",
      "       Other       0.51      0.52      0.52       202\n",
      "       B-Per       0.97      0.97      0.97     16653\n",
      "       I-Org       0.42      0.50      0.45        10\n",
      "       B-Loc       0.61      0.62      0.62       350\n",
      "\n",
      "   micro avg       0.94      0.94      0.94     18036\n",
      "   macro avg       0.54      0.55      0.55     18036\n",
      "weighted avg       0.94      0.94      0.94     18036\n",
      "\n",
      "Decision Tree F1 score: 0.94\n",
      "Results for Naive Bayes...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       I-Loc       0.07      0.24      0.11       153\n",
      "       B-Org       0.28      0.48      0.35       645\n",
      "       I-Per       0.01      0.30      0.01        23\n",
      "       Other       0.07      0.35      0.11       202\n",
      "       B-Per       0.97      0.79      0.87     16653\n",
      "       I-Org       0.00      0.00      0.00        10\n",
      "       B-Loc       0.16      0.19      0.17       350\n",
      "\n",
      "   micro avg       0.75      0.75      0.75     18036\n",
      "   macro avg       0.22      0.33      0.23     18036\n",
      "weighted avg       0.91      0.75      0.82     18036\n",
      "\n",
      "Naive Bayes F1 score: 0.69\n",
      "SVM\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       I-Loc       0.07      0.24      0.11       153\n",
      "       B-Org       0.28      0.48      0.35       645\n",
      "       I-Per       0.01      0.30      0.01        23\n",
      "       Other       0.07      0.35      0.11       202\n",
      "       B-Per       0.97      0.79      0.87     16653\n",
      "       I-Org       0.00      0.00      0.00        10\n",
      "       B-Loc       0.16      0.19      0.17       350\n",
      "\n",
      "   micro avg       0.75      0.75      0.75     18036\n",
      "   macro avg       0.22      0.33      0.23     18036\n",
      "weighted avg       0.91      0.75      0.82     18036\n",
      "\n",
      "SVM F1 score: 0.69\n",
      "Results for Random Forest...\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       I-Loc       1.00      0.01      0.01       153\n",
      "       B-Org       0.81      0.34      0.48       645\n",
      "       I-Per       0.00      0.00      0.00        23\n",
      "       Other       1.00      0.00      0.01       202\n",
      "       B-Per       0.94      1.00      0.97     16653\n",
      "       I-Org       0.00      0.00      0.00        10\n",
      "       B-Loc       0.90      0.26      0.41       350\n",
      "\n",
      "   micro avg       0.94      0.94      0.94     18036\n",
      "   macro avg       0.66      0.23      0.27     18036\n",
      "weighted avg       0.93      0.94      0.92     18036\n",
      "\n",
      "random Forest F1 score: 0.96\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/akshat14714/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/home/akshat14714/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1145: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "X = pd.read_csv('./featureVector.csv')\n",
    "\n",
    "y = X['word.Tag']\n",
    "\n",
    "# removing the Tag column from X to keep it as feature only.\n",
    "X.drop('word.Tag', axis=1, inplace=True)\n",
    "\n",
    "# handelling the NaN and inf values in the dataset\n",
    "X=X.astype('float32')\n",
    "y=y.astype('float32')\n",
    "X = np.nan_to_num(X)\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "\n",
    "dtc = DecisionTreeClassifier(max_depth=32, class_weight=dict([{0:1,1:1}, {0:1,1:50}, {0:1,1:18},{0:1,1:1940}, {0:1,1:70},{0:1,1:3},{0:1,1:25}]))\n",
    "gnb = GaussianNB()\n",
    "svm = SVC(gamma='auto')\n",
    "clf = RandomForestClassifier(max_depth=10)\n",
    "\n",
    "# fit\n",
    "dtc.fit(X_train, y_train)\n",
    "gnb.fit(X_train, y_train)\n",
    "#svm.fit(X_train, y_train)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# predict\n",
    "y_pred = dtc.predict(X_test)\n",
    "target_names = ['I-Loc', 'B-Org', 'I-Per', 'Other', 'B-Per', 'I-Org', 'B-Loc']\n",
    "\n",
    "# print\n",
    "print (\"Results for Decision tree..\")\n",
    "\n",
    "print(classification_report(y_test, y_pred, target_names=target_names))\n",
    "\n",
    "\n",
    "# f1 score\n",
    "score = f1_score(y_pred, y_test, average='weighted')\n",
    "print( \"Decision Tree F1 score: {:.2f}\".format(score))\n",
    "\n",
    "\n",
    "print (\"Results for Naive Bayes...\")\n",
    "y_pred = gnb.predict(X_test)\n",
    "print(classification_report(y_test, y_pred, target_names=target_names))\n",
    "\n",
    "# f1 score\n",
    "score = f1_score(y_pred, y_test, average='weighted')\n",
    "print (\"Naive Bayes F1 score: {:.2f}\".format(score))\n",
    "\n",
    "\n",
    "# print\n",
    "print (\"SVM\")\n",
    "\n",
    "print(classification_report(y_test, y_pred, target_names=target_names))\n",
    "\n",
    "\n",
    "# f1 score\n",
    "score = f1_score(y_pred, y_test, average='weighted')\n",
    "print( \"SVM F1 score: {:.2f}\".format(score))\n",
    "\n",
    "\n",
    "\n",
    "print( \"Results for Random Forest...\")\n",
    "y_pred = clf.predict(X_test)\n",
    "print(classification_report(y_test, y_pred, target_names=target_names))\n",
    "\n",
    "# f1 score\n",
    "score = f1_score(y_pred, y_test, average='weighted')\n",
    "print (\"random Forest F1 score: {:.2f}\".format(score))\n",
    "\n",
    "# # Cross validation on Data\n",
    "# pred = cross_val_predict(estimator=dtc, X=X, y=y, cv=5)\n",
    "# print(classification_report(pred, y, target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I-Per', 'B-Per', 'I-Org', 'B-Loc', 'Other', 'I-Loc', 'B-Org']\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       B-Loc       0.00      0.00      0.00       795\n",
      "       B-Org       0.00      0.00      0.00      1528\n",
      "       B-Per       0.03      0.42      0.05      2362\n",
      "       I-Loc       0.00      0.00      0.00        31\n",
      "       I-Org       0.00      0.00      0.00        96\n",
      "       I-Per       0.00      0.00      0.00       571\n",
      "       Other       0.91      0.50      0.65     66760\n",
      "\n",
      "   micro avg       0.48      0.48      0.48     72143\n",
      "   macro avg       0.13      0.13      0.10     72143\n",
      "weighted avg       0.85      0.48      0.60     72143\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CRF(algorithm='l2sgd', all_possible_states=None,\n",
       "  all_possible_transitions=False, averaging=None, c=None, c1=None, c2=0.1,\n",
       "  calibration_candidates=None, calibration_eta=None,\n",
       "  calibration_max_trials=None, calibration_rate=None,\n",
       "  calibration_samples=None, delta=None, epsilon=None, error_sensitive=None,\n",
       "  gamma=None, keep_tempfiles=None, linesearch=None, max_iterations=1000,\n",
       "  max_linesearch=None, min_freq=None, model_filename=None,\n",
       "  num_memories=None, pa_type=None, period=None, trainer_cls=None,\n",
       "  variance=None, verbose=False)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn_crfsuite import CRF\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn_crfsuite.metrics import flat_classification_report\n",
    "\n",
    "data = pd.read_csv(\"annotatedData.csv\", encoding=\"latin1\")\n",
    "data = data.fillna(method=\"ffill\")\n",
    "\n",
    "getter = SentenceGetter(data)\n",
    "sentences = getter.sentences\n",
    "\n",
    "X1 = numericFeatures()\n",
    "Y1 = [sent2labels(s) for s in sentences]\n",
    "\n",
    "\n",
    "\n",
    "crf = CRF(algorithm='l2sgd',\n",
    "          c2 = 0.1,\n",
    "          max_iterations = 1000,\n",
    "          all_possible_transitions = False)\n",
    "\n",
    "\n",
    "pred = cross_val_predict(estimator = crf, X = X1, y = Y1, cv = 2)\n",
    "report = flat_classification_report(y_pred = pred, y_true = Y1)\n",
    "print(report)\n",
    "\n",
    "crf.fit(X1, Y1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(72143, 32, 1)\n",
      "Train on 57714 samples, validate on 14429 samples\n",
      "Epoch 1/5\n",
      "57714/57714 [==============================] - 32s 557us/step - loss: 0.4073 - acc: 0.9161 - val_loss: 0.2988 - val_acc: 0.9409\n",
      "Epoch 2/5\n",
      "57714/57714 [==============================] - 36s 617us/step - loss: 0.3231 - acc: 0.9204 - val_loss: 0.2972 - val_acc: 0.9361\n",
      "Epoch 3/5\n",
      "57714/57714 [==============================] - 37s 633us/step - loss: 0.3107 - acc: 0.9211 - val_loss: 0.2878 - val_acc: 0.9097\n",
      "Epoch 4/5\n",
      "57714/57714 [==============================] - 33s 580us/step - loss: 0.3050 - acc: 0.9222 - val_loss: 0.2846 - val_acc: 0.9373\n",
      "Epoch 5/5\n",
      "57714/57714 [==============================] - 33s 579us/step - loss: 0.3018 - acc: 0.9224 - val_loss: 0.2719 - val_acc: 0.9394\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100)               40800     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 7)                 707       \n",
      "=================================================================\n",
      "Total params: 41,507\n",
      "Trainable params: 41,507\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Embedding, TimeDistributed, Dropout\n",
    "\n",
    "dataset = pd.read_csv('featureVector.csv', header=0)\n",
    "val = dataset.values\n",
    "val=val.astype('float32')\n",
    "val = np.nan_to_num(val)\n",
    "\n",
    "X = val[:,:32]\n",
    "Y = val[:,32]\n",
    "\n",
    "# print X.shape, Y.shape\n",
    "\n",
    "X = np.reshape(X, (X.shape[0], X.shape[1], 1))\n",
    "print(X.shape)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(LSTM(100, input_shape=(32, 1)))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(7,activation='softmax')) #7 class classification.\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics = ['accuracy'])\n",
    "model.fit(X, Y, epochs=5, batch_size=32, validation_split = 0.2, verbose=1)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 57714 samples, validate on 14429 samples\n",
      "Epoch 1/5\n",
      "57714/57714 [==============================] - 47s 817us/step - loss: 0.3597 - acc: 0.9189 - val_loss: 0.2838 - val_acc: 0.9410\n",
      "Epoch 2/5\n",
      "57714/57714 [==============================] - 43s 751us/step - loss: 0.3106 - acc: 0.9220 - val_loss: 0.2856 - val_acc: 0.9340\n",
      "Epoch 3/5\n",
      "57714/57714 [==============================] - 43s 753us/step - loss: 0.3042 - acc: 0.9218 - val_loss: 0.2752 - val_acc: 0.9392\n",
      "Epoch 4/5\n",
      "57714/57714 [==============================] - 44s 762us/step - loss: 0.3010 - acc: 0.9230 - val_loss: 0.2886 - val_acc: 0.9154\n",
      "Epoch 5/5\n",
      "57714/57714 [==============================] - 44s 764us/step - loss: 0.2980 - acc: 0.9230 - val_loss: 0.2657 - val_acc: 0.9419\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bidirectional_1 (Bidirection (None, 200)               81600     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 7)                 1407      \n",
      "=================================================================\n",
      "Total params: 83,007\n",
      "Trainable params: 83,007\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Bidirectional\n",
    "\n",
    "model1 = Sequential()\n",
    "model1.add(Bidirectional(LSTM(100, input_shape=(32, 1))))\n",
    "model1.add(Dropout(0.3))\n",
    "model1.add(Dense(7,activation='softmax')) #7 class classification.\n",
    "model1.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics = ['accuracy'])\n",
    "model1.fit(X, Y, epochs=5, batch_size=32, validation_split = 0.2, verbose=1)\n",
    "\n",
    "model1.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
